# -*- coding: utf-8 -*-
"""02_taskA_Summarisation_TrainingAndInference.ipynb

Automatically generated by Colaboratory.
"""

import pandas as pd
from simpletransformers.seq2seq import (
    Seq2SeqModel,
    Seq2SeqArgs,
)
from simpletransformers.t5 import T5Model, T5Args
import torch
import sys

train = pd.read_csv('data/TaskA-TrainingSet.csv')
test = pd.read_csv(sys.argv[1])

train['dialogue'] = train['dialogue'].map(lambda x: x.replace('\n',' ').replace('\r',' '))
test['dialogue'] = test['dialogue'].map(lambda x: x.replace('\n',' ').replace('\r',' '))

train_data = pd.DataFrame()

train_data[['input_text', 'target_text']] = train[['dialogue', 'section_text']]

train_df = pd.DataFrame(
    train_data, columns=["input_text", "target_text"]
)

"""# Training & Inference from BART Model 1 (Bart Large)"""
def train_and_infer_bart_large(train_df, test):
    
    model_args1 = Seq2SeqArgs()
    model_args1.num_train_epochs = 5
    model_args1.max_length = 256
    model_args1.num_beams = 5
    
    """
    # # Initialize model
    model1 = Seq2SeqModel(
        encoder_decoder_type="bart",
        encoder_decoder_name="facebook/bart-large",
        args=model_args1,
        use_cuda=True,
    )
    model1.train_model(train_df)
    prediction1_test = model1.predict(test['dialogue'].tolist())
    return prediction1_test
    """
    model1 = Seq2SeqModel(
        encoder_decoder_type="bart",
        encoder_decoder_name="prakhar2112/sectiontextBART",
        args=model_args1
    )
    prediction1_test = model1.predict(test['dialogue'].tolist())
    return prediction1_test

prediction1_test = train_and_infer_bart_large(train_df, test)
import pickle
pickle.dump(prediction1_test, open(f'intermediate_outputs/{sys.argv[2]}/taska_summary_bart_large_ontest.pkl', 'wb'))

"""
Training & Infernece from BART Model 2 (BioBart Large)
"""
def train_and_infer_biobart_large(train_df, test):
    
    model_args2 = Seq2SeqArgs()
    model_args2.num_train_epochs = 5
    model_args2.max_length = 256
    model_args2.num_beams = 5
    
    """
    
    # # Initialize model
    model2 = Seq2SeqModel(
        encoder_decoder_type="bart",
        encoder_decoder_name="GanjinZero/biobart-v2-large",
        args=model_args2,
        use_cuda=True,
    )
    """
    model2 = Seq2SeqModel(
        encoder_decoder_type="bart",
        encoder_decoder_name="prakhar2112/sectiontextBioBART",
        args=model_args2
    )
    prediction2_test = model2.predict(test['dialogue'].tolist())
    return prediction2_test

prediction2_test = train_and_infer_biobart_large(train_df, test)
pickle.dump(prediction2_test, open(f'intermediate_outputs/{sys.argv[2]}/taska_summary_biobart_large_ontest.pkl', 'wb'))

"""# Inference from GPT-3"""

import os
import openai

train_data = pd.read_csv('data/TaskA-TrainingSet.csv')
test_data = pd.read_csv(sys.argv[1])
section_header_predictions_test = pd.read_csv(f'intermediate_outputs/{sys.argv[2]}/final_taska_classification_output_ontest.csv')

openai.api_key = "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"

def gpt3autofill(p):
  response = openai.Completion.create(
    model="text-davinci-003",
    prompt=p,
    temperature=0.6,
    max_tokens=200,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0
  )
  return response["choices"][0]["text"]

normalizer = {
              'FAM/SOCHX': 'FAMILY HISTORY/SOCIAL HISTORY',
              'GENHX': 'HISTORY OF PRESENT ILLNESS',
              'PASTMEDICALHX': 'PAST MEDICAL HISTORY',
              'CC': 'CHIEF COMPLAINT',
              'PASTSURGICAL': 'PAST SURGICAL HISTORY',
              'ROS': 'REVIEW OF SYSTEMS',
              'EDCOURSE': 'EMERGENCY DEPARTMENT COURSE',
              'GYNHX': 'GYNECOLOGIC HISTORY'
              }
def normalize(word):
  return normalizer.get(word, word)


unq_headers = list(train_data['section_header'].unique())
label2id = {i:idx for idx,i in enumerate(unq_headers)}
id2label = {idx:i for idx,i in enumerate(unq_headers)}

import collections
Tr = collections.defaultdict(list)
for i,j,k in zip(train_data['dialogue'].tolist(), train_data['section_header'].tolist(), train_data['section_text'].tolist()):
  Tr[normalize(j)].append((i,k))

import time
import random

extra_text = {
    'ALLERGY': "Incase of no allergies, reply with with keyword 'no known allergies'",
    'FAM/SOCHX': "Incase of no family medical history found, reply with keyword 'noncontributory'",
    # "GENHX": "Don't forget to mention age and gender of the patient if present.",
    # "MEDICATIONS": "Incase the patient is not on any medication, reply with with keyword 'none'"
 }

section_text_predictions=[]
for i,j in zip(test_data['dialogue'].tolist(), section_header_predictions_test['SystemOutput1'].tolist()):
  time.sleep(5)
  preds = j
  
  coverage_text = extra_text.get(preds,'')
  prediction_section = normalize(preds)

  examples = random.choices(Tr[prediction_section],k=3)

  kg=""
  for example in examples:  
    insert = f"""
              ###

              Dialogue:
              {example[0]}

              Context: {prediction_section}

              The summary of dialogue sequence in the given context is {example[1]}

              """
    kg += insert
  
  prompt = f"""
            Act as a doctor's assistant. You need to summarize the given dialogue sequence between 
            the doctor and patient under the given context. Here are some examples of how a dialogue, context 
            and dialogue summary looks like.

            {kg}

            ###

            Now, given a dialogue sequence and dialogue context you need to generate 
            dialogue summary in the similar style as shown above. {coverage_text}

            Dialogue:
            {i}

            Context: {prediction_section}

            The summary of dialogue sequence in the given context is
  """  
  response = gpt3autofill(prompt)
  section_text_predictions.append(response.strip())
  
pickle.dump(section_text_predictions, open(f'intermediate_outputs/{sys.argv[2]}/taska_summary_gpt3_ontest.pkl', 'wb'))

